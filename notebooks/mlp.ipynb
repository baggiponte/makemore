{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b41b38e-1865-4172-a6cc-a45fe845ac7d",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/baggiponte/makemore/blob/main/notebooks/mlp.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0043ab65-1ed5-4c76-b3e9-671965a6f03e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3857e993-771b-4d30-9acc-238eb74627d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from makemore.datasets import fetch_names\n",
    "except ModuleNotFoundError:\n",
    "    !pip install --quiet -- makemore\n",
    "    from makemore.datasets import fetch_names\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504ca13b-fc7b-414e-92cf-76d349b4ca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = fetch_names(shuffle=True, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea31942-af05-49ee-8f57-2a44a16d3d61",
   "metadata": {},
   "source": [
    "# Obtain Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ad462b-2007-4f75-a064-b8fb9321ee3e",
   "metadata": {},
   "source": [
    "Here are some special parameters, called \"hyperparameters\" that you can tweak manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43348c00-3ec5-45c7-9871-0d09beb00ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 3\n",
    "\n",
    "TRAIN_SIZE = 0.8\n",
    "TEST_SIZE = 0.9\n",
    "\n",
    "EMBEDDING_DIMENSIONS = 10\n",
    "HIDDEN_LAYER_NEURONS = 200\n",
    "\n",
    "TRAINING_STEPS = 200_000\n",
    "MINIBATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91949673-69be-4149-be28-96fdbe62c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "context, labels = names.get_ngrams(CONTEXT_SIZE, as_tensor=True)\n",
    "\n",
    "training_index = int(TRAIN_SIZE*len(context))\n",
    "test_index = int(TEST_SIZE*len(context))\n",
    "\n",
    "X_train, X_validation, X_test = context[:training_index], context[training_index:test_index], context[test_index:]\n",
    "y_train, y_validation, y_test = labels[:training_index], labels[training_index:test_index], labels[test_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ae4c0e-08ac-4f6a-ad83-b3a1055dff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Train set:\\tX: {len(X_train)}\\ty:{len(y_train)}\",\n",
    "    f\"Validation set:\\tX: {len(X_validation)}\\ty:{len(y_validation)}\",\n",
    "    f\"Test set:\\tX: {len(X_test)}\\ty:{len(y_test)}\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643a1fb5-f48d-4a46-9e66-9552f1052976",
   "metadata": {},
   "source": [
    "# Split in train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afb4bab-dd67-464d-8dfa-f8cbe80cd57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(42) # for reproducibility\n",
    "\n",
    "C = torch.randn((27, EMBEDDING_DIMENSIONS), generator=g, requires_grad=True)\n",
    "\n",
    "W1 = torch.randn((CONTEXT_SIZE * EMBEDDING_DIMENSIONS, HIDDEN_LAYER_NEURONS), generator=g, requires_grad=True)\n",
    "b1 = torch.randn(HIDDEN_LAYER_NEURONS, generator=g, requires_grad=True)\n",
    "\n",
    "W2 = torch.randn((HIDDEN_LAYER_NEURONS, 27), generator=g, requires_grad=True)\n",
    "b2 = torch.randn(27, generator=g, requires_grad=True)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010588c1-120a-40f1-825b-f0616303b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216620a4-c922-4fda-8748-faa6b2139870",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e60d358-355b-4642-ae16-d206f5cae238",
   "metadata": {},
   "outputs": [],
   "source": [
    "lri = []\n",
    "lossi = []\n",
    "stepi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee57c829-12d4-441f-9612-f9a413783458",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(TRAINING_STEPS):\n",
    "\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, X_train.shape[0], (MINIBATCH_SIZE,))\n",
    "  \n",
    "    # forward pass\n",
    "    emb = C[X_train[ix]]\n",
    "    h = torch.tanh(emb.view(-1, CONTEXT_SIZE * EMBEDDING_DIMENSIONS) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "  \n",
    "    loss = F.cross_entropy(logits, y_train[ix])\n",
    "  \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{TRAINING_STEPS:7d}: {loss.item():.4f}')\n",
    "    \n",
    "    stepi.append(i)\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1ac96f-9908-42cc-8ee6-8445fa0493e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(stepi, lossi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396433bf-a208-4361-80a4-0794d88a0594",
   "metadata": {},
   "source": [
    "## Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c432a12-32d6-42ae-a0ff-abe7864557eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[X_train]\n",
    "h = torch.tanh(emb.view(-1, CONTEXT_SIZE * EMBEDDING_DIMENSIONS) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, y_train)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0071ba07-2c2c-49a1-bd0f-0a479384a1e0",
   "metadata": {},
   "source": [
    "## Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c228ddd6-23e3-4694-845e-15b8e360eafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[X_validation]\n",
    "h = torch.tanh(emb.view(-1, CONTEXT_SIZE * EMBEDDING_DIMENSIONS) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, y_validation)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57cf87f-7787-4984-9db8-e1edb84dc039",
   "metadata": {},
   "source": [
    "# Visualise Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93de080f-a8c5-4f8e-bab8-a02b713453ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from makemore.utils import INT_TO_STRING\n",
    "\n",
    "# visualize dimensions 0 and 1 of the embedding matrix C for all characters\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(C[:,0].data, C[:,1].data, s=200)\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i,0].item(), C[i,1].item(), INT_TO_STRING[i], ha=\"center\", va=\"center\", color='white')\n",
    "plt.grid('minor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c180571-3a31-4984-a96f-0a96ad0f3d32",
   "metadata": {},
   "source": [
    "# Test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd625659-b048-4bf5-ada8-2848e056dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test loss\n",
    "emb = C[X_test] # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, y_test)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfe77a0-289e-4975-9aca-67224eac496a",
   "metadata": {},
   "source": [
    "# Generate names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c2eca-ac20-486a-9acd-2e36698656b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(42 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * CONTEXT_SIZE # initialize with all ...\n",
    "    while True:\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
    "      h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "      logits = h @ W2 + b2\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(INT_TO_STRING[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
