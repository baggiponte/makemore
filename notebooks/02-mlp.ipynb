{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b41b38e-1865-4172-a6cc-a45fe845ac7d",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/baggiponte/makemore/blob/main/notebooks/02-mlp.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0043ab65-1ed5-4c76-b3e9-671965a6f03e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da1949-b765-4edd-8e0e-0a81be734ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from makemore.datasets import fetch_names\n",
    "except ModuleNotFoundError:\n",
    "    !pip install --quiet -- makemore\n",
    "    from makemore.datasets import fetch_names\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(21474483647)\n",
    "\n",
    "# get the data\n",
    "names = fetch_names(shuffle=True, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea31942-af05-49ee-8f57-2a44a16d3d61",
   "metadata": {},
   "source": [
    "# Things you can play with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ad462b-2007-4f75-a064-b8fb9321ee3e",
   "metadata": {},
   "source": [
    "Here are some special parameters, called \"hyperparameters\" that you can tweak manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43348c00-3ec5-45c7-9871-0d09beb00ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 4\n",
    "EMBEDDING_DIMS = 10\n",
    "INNER_SIZE = 200\n",
    "\n",
    "MINIBATCH_SIZE = 64\n",
    "EPOCHS = 150_000\n",
    "EPSILON = (0.1, 0.01)\n",
    "EPSILON_CUTOFF = 100_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee45017-b59e-41c3-9c68-c4b88b606287",
   "metadata": {},
   "source": [
    "# Let's create the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2726c045-1cb7-4f35-af4e-a87301d4a325",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, context_size, embedding_dimensions, hidden_size):\n",
    "        super().__init__()\n",
    "        self.vocabulary_size = 27\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dimensions = embedding_dimensions\n",
    "        \n",
    "        self.embeddings = nn.Embedding(self.vocabulary_size, self.embedding_dimensions)\n",
    "        \n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Linear(context_size * embedding_dimensions, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, self.vocabulary_size),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embeddings = self.embeddings(x).view(-1, self.context_size * self.embedding_dimensions)\n",
    "        logits = self.stack(embeddings)\n",
    "        return logits\n",
    "        \n",
    "model = EmbeddingMLP(CONTEXT_SIZE, EMBEDDING_DIMS, INNER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643a1fb5-f48d-4a46-9e66-9552f1052976",
   "metadata": {},
   "source": [
    "# Prepare the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bd333e-5f7c-4ded-a68f-65a06ce921e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "context, labels = names.get_ngrams(CONTEXT_SIZE, as_tensor=True)\n",
    "\n",
    "TRAIN_SIZE = 0.8\n",
    "TEST_SIZE = 0.9\n",
    "\n",
    "training_index = int(TRAIN_SIZE*len(context))\n",
    "test_index = int(TEST_SIZE*len(context))\n",
    "\n",
    "X_train, X_validation, X_test = context[:training_index], context[training_index:test_index], context[test_index:]\n",
    "y_train, y_validation, y_test = labels[:training_index], labels[training_index:test_index], labels[test_index:]\n",
    "\n",
    "print(\n",
    "    f\"Train set:\\tX: {len(X_train)}\\ty:{len(y_train)}\",\n",
    "    f\"Validation set:\\tX: {len(X_validation)}\\ty:{len(y_validation)}\",\n",
    "    f\"Test set:\\tX: {len(X_test)}\\ty:{len(y_test)}\",\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216620a4-c922-4fda-8748-faa6b2139870",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe821db-bd2f-4f0e-8d94-9335c7588f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lri = []\n",
    "lossi = []\n",
    "stepi = []\n",
    "\n",
    "for k in range(EPOCHS):\n",
    "    \n",
    "    # generate minibatches\n",
    "    ix = torch.randint(0, X_train.shape[0], (MINIBATCH_SIZE,))\n",
    "    \n",
    "    # forward pass\n",
    "    batch = X_train[ix]\n",
    "    \n",
    "    logits = model(batch)\n",
    "    \n",
    "    # much better in terms of performance and numerical stability\n",
    "    loss = F.cross_entropy(logits, y_train[ix])\n",
    "\n",
    "    # backward pass\n",
    "    model.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    # update weights\n",
    "    lr = EPSILON[0] if k < EPSILON_CUTOFF else EPSILON[1]\n",
    "    for p in model.parameters():\n",
    "        p.data -= lr * p.grad\n",
    "    \n",
    "    if k % 10000 == 0: # print every once in a while\n",
    "        print(f'{k:7d}/{EPOCHS:7d}: {loss.item():.4f}')\n",
    "    \n",
    "    stepi.append(k)\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "print(f\"\\nLast batch loss: {loss.item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1ac96f-9908-42cc-8ee6-8445fa0493e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(stepi, lossi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396433bf-a208-4361-80a4-0794d88a0594",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c432a12-32d6-42ae-a0ff-abe7864557eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_loss(X, y) -> None:\n",
    "    logits = model(X)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "print(\n",
    "    f\"Train loss:\\t\\t{evaluate_loss(X_train, y_train):.5f}\",\n",
    "    f\"Validation loss:\\t{evaluate_loss(X_validation, y_validation):.5f}\",\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfe77a0-289e-4975-9aca-67224eac496a",
   "metadata": {},
   "source": [
    "# Generate names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c2eca-ac20-486a-9acd-2e36698656b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from makemore.utils import int_to_character\n",
    "\n",
    "g = torch.Generator().manual_seed(21474483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * CONTEXT_SIZE\n",
    "    while True:\n",
    "        logits = model(torch.tensor([context]))\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        \n",
    "        if ix == 0:\n",
    "            break\n",
    "            \n",
    "    print(\"\".join(int_to_character(i) for i in out[:-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
